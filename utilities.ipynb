{
 "metadata": {
  "name": "",
  "signature": "sha256:26856595c253da4e203eed248feeb5328b6b2be4bd8afe6953d6aecbde703249"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skimage.filter import threshold_otsu, threshold_adaptive, gaussian_filter\n",
      "from skimage.color import color_dict, gray2rgb, label2rgb, rgb2gray\n",
      "from skimage.segmentation import clear_border\n",
      "from skimage.morphology import binary_dilation, binary_erosion, watershed, remove_small_objects\n",
      "from skimage.measure import regionprops, label\n",
      "from skimage.restoration import denoise_bilateral\n",
      "from skimage.util import img_as_ubyte\n",
      "from skimage.io import imread, imsave\n",
      "import numpy as np\n",
      "import os, csv\n",
      "\n",
      "\n",
      "def draw_arrow(image, p, q, color, arrow_magnitude=9, thickness=5, line_type=8, shift=0):\n",
      "    # adapted from http://mlikihazar.blogspot.com.au/2013/02/draw-arrow-opencv.html\n",
      "\n",
      "    import cv2\n",
      "\n",
      "    # draw arrow tail\n",
      "    cv2.line(image, p, q, color, thickness, line_type, shift)\n",
      "    # calc angle of the arrow \n",
      "    angle = np.arctan2(p[1]-q[1], p[0]-q[0])\n",
      "    # starting point of first line of arrow head \n",
      "    p = (int(q[0] + arrow_magnitude * np.cos(angle + np.pi/4)),\n",
      "    int(q[1] + arrow_magnitude * np.sin(angle + np.pi/4)))\n",
      "    # draw first half of arrow head\n",
      "    cv2.line(image, p, q, color, thickness, line_type, shift)\n",
      "    # starting point of second line of arrow head \n",
      "    p = (int(q[0] + arrow_magnitude * np.cos(angle - np.pi/4)),\n",
      "    int(q[1] + arrow_magnitude * np.sin(angle - np.pi/4)))\n",
      "    # draw second half of arrow head\n",
      "    cv2.line(image, p, q, color, thickness, line_type, shift)\n",
      "    \n",
      "def foreground_mask(img, min_size=64, thresh=200):\n",
      "    \"\"\"\n",
      "    Find the mask that covers exactly the foreground of the brain slice image.\n",
      "    This depends heavily on the manually chosen threshold, and thus is very fragile.\n",
      "    It works reasonably well on bright backgrounds, such as blue nissl images; \n",
      "    but without tuning the threshold, it does not work on images with dark background, such as fluorescent images.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    img : image\n",
      "        input image\n",
      "    min_size : float\n",
      "    thresh : float\n",
      "    \n",
      "    Return\n",
      "    ------\n",
      "    mask : \n",
      "        foreground mask\n",
      "    \"\"\"\n",
      "    \n",
      "#     t_img = gaussian_filter(img, sigma=3) < 220./255.\n",
      "    t_img = denoise_bilateral(img) < thresh/255.\n",
      "\n",
      "    labels, n_labels = label(t_img, neighbors=4, return_num=True)\n",
      "    \n",
      "    reg = regionprops(labels+1)\n",
      "    all_areas = np.array([r.area for r in reg])\n",
      "    \n",
      "    a = np.concatenate([labels[0,:] ,labels[-1,:] ,labels[:,0] ,labels[:,-1]])\n",
      "    border_labels = np.unique(a)\n",
      "    \n",
      "    border_labels_large = np.extract(all_areas[border_labels] > 250, border_labels)\n",
      "\n",
      "    mask = np.ones_like(img, dtype=np.bool)\n",
      "    for i in border_labels_large:\n",
      "        if i != all_areas.argmax():\n",
      "            mask[labels==i] = 0\n",
      "\n",
      "    mask = remove_small_objects(mask, min_size=min_size, connectivity=1, in_place=False)\n",
      "            \n",
      "    return mask\n",
      "\n",
      "from scipy.ndimage import measurements\n",
      "\n",
      "def crop_image(img, smooth=20):\n",
      "    blurred = gaussian_filter(img, smooth)\n",
      "    thresholded = blurred < threshold_otsu(blurred)\n",
      "    slc = measurements.find_objects(thresholded)[0]\n",
      "\n",
      "#     margin = 100\n",
      "#     xstart = max(slc[0].start - margin, 0)\n",
      "#     xstop = min(slc[0].stop + margin, img.shape[0])\n",
      "#     ystart = max(slc[1].start - margin, 0)\n",
      "#     ystop = min(slc[1].stop + margin, img.shape[1])\n",
      "#     cutout = img[xstart:xstop, ystart:ystop]\n",
      "    return slc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      " \n",
      "def timeit(func=None,loops=1,verbose=False):\n",
      "    if func != None:\n",
      "        def inner(*args,**kwargs):\n",
      " \n",
      "            sums = 0.0\n",
      "            mins = 1.7976931348623157e+308\n",
      "            maxs = 0.0\n",
      "            print '==== %s ====' % func.__name__\n",
      "            for i in range(0,loops):\n",
      "                t0 = time.time()\n",
      "                result = func(*args,**kwargs)\n",
      "                dt = time.time() - t0\n",
      "                mins = dt if dt < mins else mins\n",
      "                maxs = dt if dt > maxs else maxs\n",
      "                sums += dt\n",
      "                if verbose == True:\n",
      "                    print '\\t%r ran in %2.9f sec on run %s' %(func.__name__,dt,i)\n",
      "            \n",
      "            if loops == 1:\n",
      "                print '%r run time was %2.9f sec' % (func.__name__,sums)\n",
      "            else:\n",
      "                print '%r min run time was %2.9f sec' % (func.__name__,mins)\n",
      "                print '%r max run time was %2.9f sec' % (func.__name__,maxs)\n",
      "                print '%r avg run time was %2.9f sec in %s runs' % (func.__name__,sums/loops,loops)\n",
      "            \n",
      "            return result\n",
      " \n",
      "        return inner\n",
      "    else:\n",
      "        def partial_inner(func):\n",
      "            return timeit(func,loops,verbose)\n",
      "        return partial_inner"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import tables\n",
      "\n",
      "# class DataManager(object):\n",
      "\n",
      "    \n",
      "#     def __init__(self):\n",
      "#         complevel = 5\n",
      "#         filters = tables.Filters(complevel=complevel, complib='blosc')\n",
      "#         # h5file = tables.open_file(\"gabor_files.h5\", mode = \"w\")\n",
      "#         self.h5file = tables.open_file(\"results.h5\", mode = \"a\", title = \"Pipeline Results\", filters=filters)\n",
      "    \n",
      "#         self.stack_name = 'RS141'\n",
      "#         self.resolution = 'x5'\n",
      "#         self.slice_num = 1\n",
      "        \n",
      "#         class Result(tables.IsDescription):\n",
      "#             name = StringCol()\n",
      "        \n",
      "#         self.stack_group = self.h5file.create_group('/', 'RS141', 'RS141')\n",
      "#         self.resol_group = self.h5file.create_group(self.stack_group, 'x5', 'x5')\n",
      "#         self.slice_group = self.h5file.create_group(self.resol_group, 'slice001', 'slice001')\n",
      "\n",
      "#     def save_result(self, data, name):\n",
      "        \n",
      "#         self.h5file.create_carray(self.slice_group, name, obj=data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# REGENERATE_ALL_RESULTS = True\n",
      "REGENERATE_ALL_RESULTS = False\n",
      "\n",
      "import json\n",
      "import cPickle as pickle\n",
      "\n",
      "class DataManager(object):\n",
      "\n",
      "    def __init__(self, data_dir, repo_dir):\n",
      "        self.data_dir = data_dir\n",
      "        self.repo_dir = repo_dir\n",
      "        self.params_dir = os.path.join(repo_dir, 'params')\n",
      "\n",
      "        self.image_name = None\n",
      "        \n",
      "    def set_stack(self, stack, resol):\n",
      "        self.stack = stack\n",
      "        self.resol = resol\n",
      "        self.resol_dir = os.path.join(self.data_dir, self.stack, self.resol)\n",
      "        \n",
      "    def set_slice(self, slice_ind):\n",
      "        assert self.stack is not None and self.resol is not None, 'Stack is not specified'\n",
      "        self.slice_ind = slice_ind\n",
      "        self.slice_str = '%04d' % slice_ind\n",
      "        self.image_dir = os.path.join(self.data_dir, self.stack, self.resol, self.slice_str)\n",
      "        self.image_name = '_'.join([self.stack, self.resol, self.slice_str])\n",
      "\n",
      "        self.labelings_dir = os.path.join(self.image_dir, 'labelings')\n",
      "        \n",
      "#         self.results_dir = os.path.join(self.image_dir, 'pipelineResults')\n",
      "        self.results_dir = os.path.join('/home/yuncong/project/DavidData2014results', self.stack, self.slice_str)\n",
      "        \n",
      "        if not os.path.exists(self.results_dir):\n",
      "            os.makedirs(self.results_dir)\n",
      "\n",
      "    def set_image(self, stack, resol, slice_ind):\n",
      "        self.set_stack(stack, resol)\n",
      "        self.set_slice(slice_ind)\n",
      "        self._load_image()\n",
      "        \n",
      "    def _load_image(self):\n",
      "        \n",
      "        assert self.image_name is not None, 'Image is not specified'\n",
      "\n",
      "        image_filename = os.path.join(self.image_dir, self.image_name + '.tif')\n",
      "        assert os.path.exists(image_filename), \"Image '%s' does not exist\" % (self.image_name + '.tif')\n",
      "        \n",
      "        self.image = imread(image_filename, as_grey=True)\n",
      "        self.image_height, self.image_width = self.image.shape[:2]\n",
      "        \n",
      "        self.image_rgb = imread(image_filename, as_grey=False)\n",
      "\n",
      "        mask_filename = os.path.join(self.image_dir, self.image_name + '_mask.png')\n",
      "        self.mask = imread(mask_filename, as_grey=True) > 0\n",
      "        \n",
      "    def set_gabor_params(self, gabor_params_id):\n",
      "        \n",
      "        self.gabor_params_id = gabor_params_id\n",
      "        self.gabor_params = json.load(open(os.path.join(self.params_dir, 'gabor', 'gabor_' + gabor_params_id + '.json'), 'r')) if gabor_params_id is not None else None\n",
      "        self._generate_kernels(self.gabor_params)\n",
      "    \n",
      "    \n",
      "    def _generate_kernels(self, gabor_params):\n",
      "        \n",
      "        from skimage.filter import gabor_kernel\n",
      "    \n",
      "        theta_interval = gabor_params['theta_interval']\n",
      "        self.n_angle = int(180/theta_interval)\n",
      "        freq_step = gabor_params['freq_step']\n",
      "        freq_max = 1./gabor_params['min_wavelen']\n",
      "        freq_min = 1./gabor_params['max_wavelen']\n",
      "        bandwidth = gabor_params['bandwidth']\n",
      "        self.n_freq = int(np.log(freq_max/freq_min)/np.log(freq_step)) + 1\n",
      "        self.frequencies = freq_max/freq_step**np.arange(self.n_freq)\n",
      "        self.angles = np.arange(0, self.n_angle)*np.deg2rad(theta_interval)\n",
      "\n",
      "        kernels = [gabor_kernel(f, theta=t, bandwidth=bandwidth) for f in self.frequencies for t in self.angles]\n",
      "        kernels = map(np.real, kernels)\n",
      "\n",
      "        biases = np.array([k.sum() for k in kernels])\n",
      "        mean_bias = biases.mean()\n",
      "        self.kernels = [k/k.sum()*mean_bias for k in kernels] # this enforces all kernel sums to be identical, but non-zero\n",
      "\n",
      "        # kernels = [k - k.sum()/k.size for k in kernels] # this enforces all kernel sum to be zero\n",
      "\n",
      "        self.n_kernel = len(kernels)\n",
      "\n",
      "        print 'num. of kernels: %d' % (self.n_kernel)\n",
      "        print 'frequencies:', self.frequencies\n",
      "        print 'wavelength (pixels):', 1/self.frequencies\n",
      "\n",
      "        self.max_kern_size = np.max([kern.shape[0] for kern in self.kernels])\n",
      "        print 'max kernel matrix size:', self.max_kern_size\n",
      "        \n",
      "    def set_segmentation_params(self, segm_params_id):\n",
      "        \n",
      "        self.segm_params_id = segm_params_id\n",
      "        self.segm_params = json.load(open(os.path.join(self.params_dir, 'segm', 'segm_' + segm_params_id + '.json'), 'r')) if segm_params_id is not None else None\n",
      "\n",
      "    def set_vq_params(self, vq_params_id):\n",
      "        \n",
      "        self.vq_params_id = vq_params_id\n",
      "        self.vq_params = json.load(open(os.path.join(self.params_dir, 'vq', 'vq_' + vq_params_id + '.json'), 'r')) if vq_params_id is not None else None\n",
      "        \n",
      "            \n",
      "    def _get_result_filename(self, result_name, ext, results_dir=None, param_dependencies=None):\n",
      "\n",
      "        results_dir = self.results_dir\n",
      "        \n",
      "        if result_name in ['features', 'kernels', 'features_rotated', 'features_rotated_pca', 'max_angle_indices']:\n",
      "            param_dependencies = ['gabor']\n",
      "\n",
      "        elif result_name in['segmentation', 'segmentationWithText', 'segmentationWithoutText',\n",
      "                            'segmentationTransparent', 'spProps', 'neighbors']:\n",
      "            param_dependencies = ['segm']\n",
      "                        \n",
      "        elif result_name in ['dirMap', 'dirHist', 'spMaxDirInd', 'spMaxDirAngle']:\n",
      "            param_dependencies = ['gabor', 'segm']\n",
      "            \n",
      "        elif result_name in ['textons']:\n",
      "            results_dir = os.path.join('/home/yuncong/project/DavidData2014results', self.stack)\n",
      "#             results_dir = self.resol_dir\n",
      "            param_dependencies = ['gabor', 'vq']\n",
      "            \n",
      "        elif result_name in ['texMap', 'original_centroids']:\n",
      "            param_dependencies = ['gabor', 'vq']\n",
      "\n",
      "        elif result_name in ['texHist', 'clusters', 'groups', 'groupsTop30Vis', 'texHistPairwiseDist']:\n",
      "            param_dependencies = ['gabor', 'segm', 'vq']\n",
      "            \n",
      "        # elif result_name == 'tmp':\n",
      "        #     results_dir = '/tmp'\n",
      "        #     instance_name = 'test'\n",
      "        \n",
      "        # elif result_name == 'models':\n",
      "        #     results_dir = self.resol_dir\n",
      "        #     instance_name = '_'.join([self.stack, self.resol,\n",
      "        #                               'gabor-' + self.gabor_params_id + '-segm-' + self.segm_params_id + \\\n",
      "        #                               '-vq-' + self.vq_params_id])\n",
      "        \n",
      "        else:\n",
      "            raise Exception('result name %s unknown' % result_name)\n",
      "\n",
      "        # instance_name = self.image_name\n",
      "\n",
      "        param_strs = []\n",
      "        if 'gabor' in param_dependencies:\n",
      "            param_strs.append('gabor-' + self.gabor_params_id)\n",
      "        if 'segm' in param_dependencies:\n",
      "            param_strs.append('segm-' + self.segm_params_id)\n",
      "        if 'vq' in param_dependencies:\n",
      "            param_strs.append('vq-' + self.vq_params_id)\n",
      "            # raise Exception(\"parameter dependency string not recognized\")\n",
      "        \n",
      "        if result_name in ['textons']:\n",
      "            result_filename = os.path.join(results_dir, self.stack + '_' +self.resol + '_' + '-'.join(param_strs) + '_' + result_name + '.' + ext)\n",
      "        else:\n",
      "            result_filename = os.path.join(results_dir, self.image_name + '_' + '-'.join(param_strs) + '_' + result_name + '.' + ext)\n",
      "        \n",
      "        return result_filename\n",
      "            \n",
      "    def load_pipeline_result(self, result_name, ext, is_rgb=None):\n",
      "        \n",
      "        if REGENERATE_ALL_RESULTS:\n",
      "            raise\n",
      "        \n",
      "        result_filename = self._get_result_filename(result_name, ext)\n",
      "        print result_filename\n",
      "\n",
      "        if ext == 'npy':\n",
      "            assert os.path.exists(result_filename), \"Pipeline result '%s' does not exist\" % (result_name + '.' + ext)\n",
      "            data = np.load(result_filename)\n",
      "        elif ext == 'tif' or ext == 'png' or ext == 'jpg':\n",
      "            data = imread(result_filename, as_grey=False)\n",
      "#             if data.ndim == 3:\n",
      "#                 data = data[...,::-1]\n",
      "            data = self._regulate_image(data, is_rgb)\n",
      "        elif ext == 'pkl':\n",
      "            data = pickle.load(open(result_filename, 'r'))\n",
      "\n",
      "        print 'loaded %s' % result_filename\n",
      "\n",
      "        return data\n",
      "        \n",
      "    def save_pipeline_result(self, data, result_name, ext, is_rgb=None):\n",
      "            \n",
      "        result_filename = self._get_result_filename(result_name, ext)\n",
      "\n",
      "        if ext == 'npy':\n",
      "            np.save(result_filename, data)\n",
      "        elif ext == 'tif' or ext == 'png' or ext == 'jpg':\n",
      "            data = self._regulate_image(data, is_rgb)\n",
      "#             if data.ndim == 3:\n",
      "#                 imsave(result_filename, data[..., ::-1])\n",
      "#             else:\n",
      "            imsave(result_filename, data)\n",
      "        elif ext == 'pkl':\n",
      "            pickle.dump(data, open(result_filename, 'w'))\n",
      "            \n",
      "        print 'saved %s' % result_filename\n",
      "        \n",
      "        \n",
      "    def save_labeling(self, labeling, new_labeling_name, labelmap_vis):\n",
      "        \n",
      "        new_labeling_fn = os.path.join(self.labelings_dir, self.image_name + '_' + new_labeling_name + '.pkl')\n",
      "        pickle.dump(labeling, open(new_labeling_fn, 'w'))\n",
      "        print 'Labeling saved to', new_labeling_fn\n",
      "\n",
      "        new_preview_fn = os.path.join(self.labelings_dir, self.image_name + '_' + new_labeling_name + '.tif')\n",
      "        \n",
      "        data = self._regulate_image(labelmap_vis, is_rgb=True)\n",
      "#         if data.ndim == 3:\n",
      "#             imsave(new_preview_fn, data[..., ::-1])\n",
      "#         else:\n",
      "        imsave(new_preview_fn, data)\n",
      "        print 'Preview saved to', new_preview_fn\n",
      "        \n",
      "    def _regulate_image(self, img, is_rgb=None):\n",
      "        \"\"\"\n",
      "        Ensure the image is of type uint8.\n",
      "        \"\"\"\n",
      "\n",
      "        if not np.issubsctype(img, np.uint8):\n",
      "            try:\n",
      "                img = img_as_ubyte(img)\n",
      "            except:\n",
      "                img_norm = (img-img.min()).astype(np.float)/(img.max() - img.min())    \n",
      "                img = img_as_ubyte(img_norm)\n",
      "\n",
      "        if is_rgb is not None:\n",
      "            if img.ndim == 2 and is_rgb:\n",
      "                img = gray2rgb(img)\n",
      "            elif img.ndim == 3 and not is_rgb:\n",
      "                img = rgb2gray(img)\n",
      "\n",
      "        return img\n",
      "    \n",
      "    \n",
      "    def load_labeling(self, labeling_name):\n",
      "        labeling_fn = os.path.join(self.labelings_dir, self.image_name + '_' + labeling_name + '.pkl')\n",
      "        labeling = pickle.load(open(labeling_fn, 'r'))\n",
      "        return labeling"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def display(vis, filename='tmp.jpg'):\n",
      "    \n",
      "    if vis.dtype != np.uint8:\n",
      "#         if vis.ndim == 3:\n",
      "#             imwrite(filename, img_as_ubyte(vis)[..., ::-1])\n",
      "#         else:\n",
      "        imwrite(filename, img_as_ubyte(vis))\n",
      "    else:\n",
      "#         if vis.ndim == 3:\n",
      "#             imwrite(filename, vis[..., ::-1])\n",
      "#         else:\n",
      "        imwrite(filename, vis)\n",
      "            \n",
      "    from IPython.display import FileLink\n",
      "    return FileLink(filename)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.ndimage.filters import maximum_filter\n",
      "from scipy.ndimage.morphology import generate_binary_structure, binary_erosion\n",
      "\n",
      "def detect_peaks(image):\n",
      "    \"\"\"\n",
      "    Takes an image and detect the peaks usingthe local maximum filter.\n",
      "    Returns a boolean mask of the peaks (i.e. 1 when\n",
      "    the pixel's value is the neighborhood maximum, 0 otherwise)\n",
      "    \"\"\"\n",
      "\n",
      "    # define an 8-connected neighborhood\n",
      "    neighborhood = generate_binary_structure(2,2)\n",
      "\n",
      "    #apply the local maximum filter; all pixel of maximal value \n",
      "    #in their neighborhood are set to 1\n",
      "    local_max = maximum_filter(image, footprint=neighborhood)==image\n",
      "    #local_max is a mask that contains the peaks we are \n",
      "    #looking for, but also the background.\n",
      "    #In order to isolate the peaks we must remove the background from the mask.\n",
      "\n",
      "    #we create the mask of the background\n",
      "    background = (image==0)\n",
      "\n",
      "    #a little technicality: we must erode the background in order to \n",
      "    #successfully subtract it form local_max, otherwise a line will \n",
      "    #appear along the background border (artifact of the local maximum filter)\n",
      "    eroded_background = binary_erosion(background, structure=neighborhood, border_value=1)\n",
      "\n",
      "    #we obtain the final mask, containing only peaks, \n",
      "    #by removing the background from the local_max mask\n",
      "    detected_peaks = local_max - eroded_background\n",
      "\n",
      "    return detected_peaks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def visualize_cluster(scores, cluster='all', title='', filename=None):\n",
      "#     '''\n",
      "#     Generate black and white image with the cluster of superpixels highlighted\n",
      "#     '''\n",
      "    \n",
      "#     vis = scores[segmentation]\n",
      "#     if cluster != 'all':\n",
      "#         cluster_selection = np.equal.outer(segmentation, cluster).any(axis=2)\n",
      "#         vis[~cluster_selection] = 0\n",
      "    \n",
      "#     plt.matshow(vis, cmap=plt.cm.Greys_r);\n",
      "#     plt.axis('off');\n",
      "#     plt.title(title)\n",
      "#     if filename is not None:\n",
      "#         plt.savefig(os.path.join(result_dir, 'stages', filename + '.png'), bbox_inches='tight')\n",
      "# #     plt.show()\n",
      "#     plt.close();\n",
      "    \n",
      "\n",
      "def paint_superpixels_on_image(superpixels, segmentation, img):\n",
      "    '''\n",
      "    Highlight a cluster of superpixels on the real image\n",
      "    '''    \n",
      "\n",
      "    cluster_map = -1*np.ones_like(segmentation)\n",
      "    for s in superpixels:\n",
      "        cluster_map[segmentation==s] = 1\n",
      "    vis = label2rgb(cluster_map, image=img)\n",
      "    return vis\n",
      "    \n",
      "def paint_superpixel_groups_on_image(sp_groups, segmentation, img, colors):\n",
      "    '''\n",
      "    Highlight multiple superpixel groups with different colors on the real image\n",
      "    '''\n",
      "    \n",
      "    cluster_map = -1*np.ones_like(segmentation)\n",
      "    for i, sp_group in enumerate(sp_groups):\n",
      "        for j in sp_group:\n",
      "            cluster_map[segmentation==j] = i\n",
      "    vis = label2rgb(cluster_map, image=img, colors=colors)\n",
      "    return vis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kl(a,b):\n",
      "    m = (a!=0) & (b!=0)\n",
      "    return np.sum(a[m]*np.log(a[m]/b[m]))\n",
      "\n",
      "def js(u,v):\n",
      "    m = .5 * (u + v)\n",
      "    r = .5 * (kl(u,m) + kl(v,m))\n",
      "    return r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chi2(u,v):\n",
      "    \"\"\"\n",
      "    Compute Chi^2 distance between two distributions.\n",
      "    \n",
      "    Empty bins are ignored.\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "#     m = (u != 0) & (v != 0)\n",
      "#     q = (u-v)**2/(u+v)\n",
      "#     r = np.sum(q[m])\n",
      "    \n",
      "    r = np.nansum((u-v)**2/(u+v))\n",
      "    return r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from mpl_toolkits.axes_grid1 import ImageGrid\n",
      "\n",
      "# def image_grid(images, ):\n",
      "#     ncols = 12\n",
      "#     nrows = n_images/ncols+1\n",
      "\n",
      "#     fig = plt.figure(1, figsize=(20., 20./ncols*nrows))\n",
      "#     grid = ImageGrid(fig, 111, # similar to subplot(111)\n",
      "#                     nrows_ncols = (nrows, ncols), # creates 2x2 grid of axes\n",
      "#                     axes_pad=0.1, # pad between axes in inch.\n",
      "#                     )\n",
      "\n",
      "#     for i in bbox.iterkeys():\n",
      "#         y1, x1, y2, x2 = bbox[i]\n",
      "#         grid[i].imshow(images[i][y1:y2, x1:x2], cmap=plt.cm.Greys_r, aspect='auto');\n",
      "#         grid[i].set_title(i)\n",
      "#         grid[i].axis('off')\n",
      "\n",
      "#     plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}